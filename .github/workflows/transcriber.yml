name: Meeting Transcriber

on:
  schedule:
    - cron: "0 0 * * *"
  workflow_dispatch:

jobs:
  transcribe:
    runs-on: ubuntu-latest

    env:
      API_SECRET: ${{ secrets.API_SECRET }}
      APPSCRIPT_URL: ${{ secrets.APPSCRIPT_URL }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: recursive

      # Cache Whisper build
      - name: Cache whisper.cpp build
        id: cache-whisper
        uses: actions/cache@v3
        with:
          path: whisper.cpp/build
          key: whisper-build-${{ hashFiles('whisper.cpp/**') }}

      # Cache Llama build
      - name: Cache llama.cpp build
        id: cache-llama
        uses: actions/cache@v3
        with:
          path: llama.cpp/build
          key: llama-build-${{ hashFiles('llama.cpp/**') }}

      # Cache models
      - name: Cache models
        id: cache-models
        uses: actions/cache@v3
        with:
          path: models
          key: models-cache-v1

      # Install system dependencies
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            build-essential \
            cmake \
            python3 \
            python3-pip \
            wget \
            unzip \
            ffmpeg

      # Build Whisper only if cache missed
      - name: Build whisper.cpp
        if: steps.cache-whisper.outputs.cache-hit != 'true'
        run: |
          cd whisper.cpp
          make

      - name: List whisper build output
        run: ls -R whisper.cpp/build/bin

      # Build Llama CLI only if cache missed
      - name: Build llama.cpp
        if: steps.cache-llama.outputs.cache-hit != 'true'
        run: |
          cd llama.cpp
          mkdir -p build
          cd build
          cmake .. -DLLAMA_CURL=OFF -DBUILD_LLAMA_CLI=ON
          cmake --build . --config Release

      - name: Show llama-cli help
        run: llama.cpp/build/bin/llama-cli --help || true



      # Download Whisper model only if cache missed
      - name: Download Whisper model
        if: steps.cache-models.outputs.cache-hit != 'true'
        run: |
          mkdir -p models
          wget -O models/ggml-base.en.bin \
            https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin

      # Download Llama model only if cache missed
      - name: Download Llama 2 7B Chat GGUF
        if: steps.cache-models.outputs.cache-hit != 'true'
        run: |
          mkdir -p models
          wget -O models/llama-2-7b-chat.Q4_K_M.gguf \
            https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf

      # Cache pip
      - name: Cache pip
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: pip-cache-v1

      - name: Install Python packages
        run: pip3 install requests

      - name: Run worker
        run: python3 main.py
